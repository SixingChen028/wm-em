{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "\n",
    "key_size = 2\n",
    "memory_size = num_points * 2\n",
    "value_size = 2\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class 0 and 1\n",
    "data_class0 = np.random.rand(num_points, key_size)\n",
    "data_class1 = np.random.rand(num_points, key_size)\n",
    "labels_class0 = np.hstack([np.zeros((num_points, 1)), np.ones((num_points, 1))])\n",
    "labels_class1 = np.hstack([np.ones((num_points, 1)), np.zeros((num_points, 1))])\n",
    "\n",
    "plt.figure(figsize = (3, 3))\n",
    "plt.scatter(data_class0[:, 0], data_class0[:, 1], label = 'class 0')\n",
    "plt.scatter(data_class1[:, 0], data_class1[:, 1], label = 'class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "data.append(data_class0)\n",
    "labels.append(labels_class0)\n",
    "\n",
    "data.append(data_class1)\n",
    "labels.append(labels_class1)\n",
    "\n",
    "# combine data and labels\n",
    "data = np.vstack(data)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "data_tensor = torch.tensor(data, dtype = torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial W_keys and W_values\n",
    "init_W_keys = data_tensor.clone()\n",
    "init_W_values = torch.tensor(np.random.rand(memory_size, value_size), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyValueMemory(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            key_size,\n",
    "            memory_size,\n",
    "            value_size,\n",
    "            W_keys,\n",
    "            W_values,\n",
    "        ):\n",
    "        super(KeyValueMemory, self).__init__()\n",
    "        self.key_size = key_size\n",
    "        self.memory_size = memory_size\n",
    "        self.value_size = value_size\n",
    "        self.W_keys = nn.Parameter(W_keys) # (memory_size, key_size)\n",
    "        self.W_values = nn.Parameter(W_values) # (memory_size, value_size)\n",
    "\n",
    "    def forward(self):\n",
    "        # compute similarity scores\n",
    "        attention_weights = torch.matmul(self.W_keys, self.W_keys.T) / self.key_size ** 0.5 # (memory_size, memory_size)\n",
    "        attention_weights = torch.softmax(attention_weights, dim = -1)\n",
    "        output = torch.matmul(attention_weights, torch.sigmoid(self.W_values)) # (memory_size, value_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sixingchen/Desktop/Codes/Project_Sampling_RNN/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50000], Loss: 0.2630\n",
      "Epoch [100/50000], Loss: 0.2622\n",
      "Epoch [150/50000], Loss: 0.2613\n",
      "Epoch [200/50000], Loss: 0.2606\n",
      "Epoch [250/50000], Loss: 0.2598\n",
      "Epoch [300/50000], Loss: 0.2591\n",
      "Epoch [350/50000], Loss: 0.2584\n",
      "Epoch [400/50000], Loss: 0.2577\n",
      "Epoch [450/50000], Loss: 0.2571\n",
      "Epoch [500/50000], Loss: 0.2565\n",
      "Epoch [550/50000], Loss: 0.2560\n",
      "Epoch [600/50000], Loss: 0.2554\n",
      "Epoch [650/50000], Loss: 0.2548\n",
      "Epoch [700/50000], Loss: 0.2543\n",
      "Epoch [750/50000], Loss: 0.2538\n",
      "Epoch [800/50000], Loss: 0.2532\n",
      "Epoch [850/50000], Loss: 0.2526\n",
      "Epoch [900/50000], Loss: 0.2520\n",
      "Epoch [950/50000], Loss: 0.2513\n",
      "Epoch [1000/50000], Loss: 0.2506\n",
      "Epoch [1050/50000], Loss: 0.2498\n",
      "Epoch [1100/50000], Loss: 0.2489\n",
      "Epoch [1150/50000], Loss: 0.2478\n",
      "Epoch [1200/50000], Loss: 0.2466\n",
      "Epoch [1250/50000], Loss: 0.2452\n",
      "Epoch [1300/50000], Loss: 0.2435\n",
      "Epoch [1350/50000], Loss: 0.2416\n",
      "Epoch [1400/50000], Loss: 0.2393\n",
      "Epoch [1450/50000], Loss: 0.2367\n",
      "Epoch [1500/50000], Loss: 0.2337\n",
      "Epoch [1550/50000], Loss: 0.2303\n",
      "Epoch [1600/50000], Loss: 0.2266\n",
      "Epoch [1650/50000], Loss: 0.2226\n",
      "Epoch [1700/50000], Loss: 0.2183\n",
      "Epoch [1750/50000], Loss: 0.2137\n",
      "Epoch [1800/50000], Loss: 0.2091\n",
      "Epoch [1850/50000], Loss: 0.2043\n",
      "Epoch [1900/50000], Loss: 0.1994\n",
      "Epoch [1950/50000], Loss: 0.1945\n",
      "Epoch [2000/50000], Loss: 0.1896\n",
      "Epoch [2050/50000], Loss: 0.1847\n",
      "Epoch [2100/50000], Loss: 0.1800\n",
      "Epoch [2150/50000], Loss: 0.1753\n",
      "Epoch [2200/50000], Loss: 0.1708\n",
      "Epoch [2250/50000], Loss: 0.1664\n",
      "Epoch [2300/50000], Loss: 0.1621\n",
      "Epoch [2350/50000], Loss: 0.1580\n",
      "Epoch [2400/50000], Loss: 0.1541\n",
      "Epoch [2450/50000], Loss: 0.1503\n",
      "Epoch [2500/50000], Loss: 0.1466\n",
      "Epoch [2550/50000], Loss: 0.1431\n",
      "Epoch [2600/50000], Loss: 0.1398\n",
      "Epoch [2650/50000], Loss: 0.1365\n",
      "Epoch [2700/50000], Loss: 0.1334\n",
      "Epoch [2750/50000], Loss: 0.1304\n",
      "Epoch [2800/50000], Loss: 0.1276\n",
      "Epoch [2850/50000], Loss: 0.1248\n",
      "Epoch [2900/50000], Loss: 0.1221\n",
      "Epoch [2950/50000], Loss: 0.1196\n",
      "Epoch [3000/50000], Loss: 0.1171\n",
      "Epoch [3050/50000], Loss: 0.1147\n",
      "Epoch [3100/50000], Loss: 0.1124\n",
      "Epoch [3150/50000], Loss: 0.1102\n",
      "Epoch [3200/50000], Loss: 0.1080\n",
      "Epoch [3250/50000], Loss: 0.1059\n",
      "Epoch [3300/50000], Loss: 0.1039\n",
      "Epoch [3350/50000], Loss: 0.1020\n",
      "Epoch [3400/50000], Loss: 0.1001\n",
      "Epoch [3450/50000], Loss: 0.0982\n",
      "Epoch [3500/50000], Loss: 0.0964\n",
      "Epoch [3550/50000], Loss: 0.0947\n",
      "Epoch [3600/50000], Loss: 0.0930\n",
      "Epoch [3650/50000], Loss: 0.0913\n",
      "Epoch [3700/50000], Loss: 0.0897\n",
      "Epoch [3750/50000], Loss: 0.0881\n",
      "Epoch [3800/50000], Loss: 0.0866\n",
      "Epoch [3850/50000], Loss: 0.0851\n",
      "Epoch [3900/50000], Loss: 0.0837\n",
      "Epoch [3950/50000], Loss: 0.0823\n",
      "Epoch [4000/50000], Loss: 0.0809\n",
      "Epoch [4050/50000], Loss: 0.0796\n",
      "Epoch [4100/50000], Loss: 0.0783\n",
      "Epoch [4150/50000], Loss: 0.0770\n",
      "Epoch [4200/50000], Loss: 0.0758\n",
      "Epoch [4250/50000], Loss: 0.0746\n",
      "Epoch [4300/50000], Loss: 0.0734\n",
      "Epoch [4350/50000], Loss: 0.0723\n",
      "Epoch [4400/50000], Loss: 0.0712\n",
      "Epoch [4450/50000], Loss: 0.0701\n",
      "Epoch [4500/50000], Loss: 0.0691\n",
      "Epoch [4550/50000], Loss: 0.0681\n",
      "Epoch [4600/50000], Loss: 0.0671\n",
      "Epoch [4650/50000], Loss: 0.0661\n",
      "Epoch [4700/50000], Loss: 0.0652\n",
      "Epoch [4750/50000], Loss: 0.0643\n",
      "Epoch [4800/50000], Loss: 0.0634\n",
      "Epoch [4850/50000], Loss: 0.0625\n",
      "Epoch [4900/50000], Loss: 0.0617\n",
      "Epoch [4950/50000], Loss: 0.0609\n",
      "Epoch [5000/50000], Loss: 0.0601\n",
      "Epoch [5050/50000], Loss: 0.0593\n",
      "Epoch [5100/50000], Loss: 0.0586\n",
      "Epoch [5150/50000], Loss: 0.0578\n",
      "Epoch [5200/50000], Loss: 0.0571\n",
      "Epoch [5250/50000], Loss: 0.0564\n",
      "Epoch [5300/50000], Loss: 0.0558\n",
      "Epoch [5350/50000], Loss: 0.0551\n",
      "Epoch [5400/50000], Loss: 0.0544\n",
      "Epoch [5450/50000], Loss: 0.0538\n",
      "Epoch [5500/50000], Loss: 0.0532\n",
      "Epoch [5550/50000], Loss: 0.0526\n",
      "Epoch [5600/50000], Loss: 0.0520\n",
      "Epoch [5650/50000], Loss: 0.0515\n",
      "Epoch [5700/50000], Loss: 0.0509\n",
      "Epoch [5750/50000], Loss: 0.0504\n",
      "Epoch [5800/50000], Loss: 0.0498\n",
      "Epoch [5850/50000], Loss: 0.0493\n",
      "Epoch [5900/50000], Loss: 0.0488\n",
      "Epoch [5950/50000], Loss: 0.0483\n",
      "Epoch [6000/50000], Loss: 0.0479\n",
      "Epoch [6050/50000], Loss: 0.0474\n",
      "Epoch [6100/50000], Loss: 0.0469\n",
      "Epoch [6150/50000], Loss: 0.0465\n",
      "Epoch [6200/50000], Loss: 0.0461\n",
      "Epoch [6250/50000], Loss: 0.0456\n",
      "Epoch [6300/50000], Loss: 0.0452\n",
      "Epoch [6350/50000], Loss: 0.0448\n",
      "Epoch [6400/50000], Loss: 0.0444\n",
      "Epoch [6450/50000], Loss: 0.0440\n",
      "Epoch [6500/50000], Loss: 0.0437\n",
      "Epoch [6550/50000], Loss: 0.0433\n",
      "Epoch [6600/50000], Loss: 0.0429\n",
      "Epoch [6650/50000], Loss: 0.0426\n",
      "Epoch [6700/50000], Loss: 0.0422\n",
      "Epoch [6750/50000], Loss: 0.0419\n",
      "Epoch [6800/50000], Loss: 0.0416\n",
      "Epoch [6850/50000], Loss: 0.0413\n",
      "Epoch [6900/50000], Loss: 0.0409\n",
      "Epoch [6950/50000], Loss: 0.0406\n",
      "Epoch [7000/50000], Loss: 0.0403\n",
      "Epoch [7050/50000], Loss: 0.0400\n",
      "Epoch [7100/50000], Loss: 0.0397\n",
      "Epoch [7150/50000], Loss: 0.0395\n",
      "Epoch [7200/50000], Loss: 0.0392\n",
      "Epoch [7250/50000], Loss: 0.0389\n",
      "Epoch [7300/50000], Loss: 0.0387\n",
      "Epoch [7350/50000], Loss: 0.0384\n",
      "Epoch [7400/50000], Loss: 0.0381\n",
      "Epoch [7450/50000], Loss: 0.0379\n",
      "Epoch [7500/50000], Loss: 0.0377\n",
      "Epoch [7550/50000], Loss: 0.0374\n",
      "Epoch [7600/50000], Loss: 0.0372\n",
      "Epoch [7650/50000], Loss: 0.0370\n",
      "Epoch [7700/50000], Loss: 0.0367\n",
      "Epoch [7750/50000], Loss: 0.0365\n",
      "Epoch [7800/50000], Loss: 0.0363\n",
      "Epoch [7850/50000], Loss: 0.0361\n",
      "Epoch [7900/50000], Loss: 0.0359\n",
      "Epoch [7950/50000], Loss: 0.0357\n",
      "Epoch [8000/50000], Loss: 0.0355\n",
      "Epoch [8050/50000], Loss: 0.0353\n",
      "Epoch [8100/50000], Loss: 0.0351\n",
      "Epoch [8150/50000], Loss: 0.0349\n",
      "Epoch [8200/50000], Loss: 0.0347\n",
      "Epoch [8250/50000], Loss: 0.0346\n",
      "Epoch [8300/50000], Loss: 0.0344\n",
      "Epoch [8350/50000], Loss: 0.0342\n",
      "Epoch [8400/50000], Loss: 0.0341\n",
      "Epoch [8450/50000], Loss: 0.0339\n",
      "Epoch [8500/50000], Loss: 0.0337\n",
      "Epoch [8550/50000], Loss: 0.0336\n",
      "Epoch [8600/50000], Loss: 0.0334\n",
      "Epoch [8650/50000], Loss: 0.0333\n",
      "Epoch [8700/50000], Loss: 0.0331\n",
      "Epoch [8750/50000], Loss: 0.0330\n",
      "Epoch [8800/50000], Loss: 0.0328\n",
      "Epoch [8850/50000], Loss: 0.0327\n",
      "Epoch [8900/50000], Loss: 0.0325\n",
      "Epoch [8950/50000], Loss: 0.0324\n",
      "Epoch [9000/50000], Loss: 0.0322\n",
      "Epoch [9050/50000], Loss: 0.0321\n",
      "Epoch [9100/50000], Loss: 0.0320\n",
      "Epoch [9150/50000], Loss: 0.0318\n",
      "Epoch [9200/50000], Loss: 0.0317\n",
      "Epoch [9250/50000], Loss: 0.0316\n",
      "Epoch [9300/50000], Loss: 0.0315\n",
      "Epoch [9350/50000], Loss: 0.0313\n",
      "Epoch [9400/50000], Loss: 0.0312\n",
      "Epoch [9450/50000], Loss: 0.0311\n",
      "Epoch [9500/50000], Loss: 0.0310\n",
      "Epoch [9550/50000], Loss: 0.0308\n",
      "Epoch [9600/50000], Loss: 0.0307\n",
      "Epoch [9650/50000], Loss: 0.0306\n",
      "Epoch [9700/50000], Loss: 0.0305\n",
      "Epoch [9750/50000], Loss: 0.0303\n",
      "Epoch [9800/50000], Loss: 0.0302\n",
      "Epoch [9850/50000], Loss: 0.0301\n",
      "Epoch [9900/50000], Loss: 0.0300\n",
      "Epoch [9950/50000], Loss: 0.0298\n",
      "Epoch [10000/50000], Loss: 0.0297\n",
      "Epoch [10050/50000], Loss: 0.0296\n",
      "Epoch [10100/50000], Loss: 0.0294\n",
      "Epoch [10150/50000], Loss: 0.0293\n",
      "Epoch [10200/50000], Loss: 0.0291\n",
      "Epoch [10250/50000], Loss: 0.0289\n",
      "Epoch [10300/50000], Loss: 0.0288\n",
      "Epoch [10350/50000], Loss: 0.0286\n",
      "Epoch [10400/50000], Loss: 0.0283\n",
      "Epoch [10450/50000], Loss: 0.0281\n",
      "Epoch [10500/50000], Loss: 0.0279\n",
      "Epoch [10550/50000], Loss: 0.0276\n",
      "Epoch [10600/50000], Loss: 0.0273\n",
      "Epoch [10650/50000], Loss: 0.0271\n",
      "Epoch [10700/50000], Loss: 0.0268\n",
      "Epoch [10750/50000], Loss: 0.0265\n",
      "Epoch [10800/50000], Loss: 0.0263\n",
      "Epoch [10850/50000], Loss: 0.0260\n",
      "Epoch [10900/50000], Loss: 0.0258\n",
      "Epoch [10950/50000], Loss: 0.0256\n",
      "Epoch [11000/50000], Loss: 0.0254\n",
      "Epoch [11050/50000], Loss: 0.0252\n",
      "Epoch [11100/50000], Loss: 0.0250\n",
      "Epoch [11150/50000], Loss: 0.0249\n",
      "Epoch [11200/50000], Loss: 0.0247\n",
      "Epoch [11250/50000], Loss: 0.0245\n",
      "Epoch [11300/50000], Loss: 0.0243\n",
      "Epoch [11350/50000], Loss: 0.0241\n",
      "Epoch [11400/50000], Loss: 0.0239\n",
      "Epoch [11450/50000], Loss: 0.0237\n",
      "Epoch [11500/50000], Loss: 0.0235\n",
      "Epoch [11550/50000], Loss: 0.0233\n",
      "Epoch [11600/50000], Loss: 0.0230\n",
      "Epoch [11650/50000], Loss: 0.0228\n",
      "Epoch [11700/50000], Loss: 0.0225\n",
      "Epoch [11750/50000], Loss: 0.0223\n",
      "Epoch [11800/50000], Loss: 0.0221\n",
      "Epoch [11850/50000], Loss: 0.0219\n",
      "Epoch [11900/50000], Loss: 0.0218\n",
      "Epoch [11950/50000], Loss: 0.0216\n",
      "Epoch [12000/50000], Loss: 0.0215\n",
      "Epoch [12050/50000], Loss: 0.0213\n",
      "Epoch [12100/50000], Loss: 0.0212\n",
      "Epoch [12150/50000], Loss: 0.0211\n",
      "Epoch [12200/50000], Loss: 0.0210\n",
      "Epoch [12250/50000], Loss: 0.0210\n",
      "Epoch [12300/50000], Loss: 0.0209\n",
      "Epoch [12350/50000], Loss: 0.0208\n",
      "Epoch [12400/50000], Loss: 0.0208\n",
      "Epoch [12450/50000], Loss: 0.0207\n",
      "Epoch [12500/50000], Loss: 0.0207\n",
      "Epoch [12550/50000], Loss: 0.0206\n",
      "Epoch [12600/50000], Loss: 0.0206\n",
      "Epoch [12650/50000], Loss: 0.0205\n",
      "Epoch [12700/50000], Loss: 0.0205\n",
      "Epoch [12750/50000], Loss: 0.0205\n",
      "Epoch [12800/50000], Loss: 0.0204\n",
      "Epoch [12850/50000], Loss: 0.0204\n",
      "Epoch [12900/50000], Loss: 0.0204\n",
      "Epoch [12950/50000], Loss: 0.0203\n",
      "Epoch [13000/50000], Loss: 0.0203\n",
      "Epoch [13050/50000], Loss: 0.0203\n",
      "Epoch [13100/50000], Loss: 0.0202\n",
      "Epoch [13150/50000], Loss: 0.0202\n",
      "Epoch [13200/50000], Loss: 0.0202\n",
      "Epoch [13250/50000], Loss: 0.0202\n",
      "Epoch [13300/50000], Loss: 0.0201\n",
      "Epoch [13350/50000], Loss: 0.0201\n",
      "Epoch [13400/50000], Loss: 0.0201\n",
      "Epoch [13450/50000], Loss: 0.0201\n",
      "Epoch [13500/50000], Loss: 0.0201\n",
      "Epoch [13550/50000], Loss: 0.0200\n",
      "Epoch [13600/50000], Loss: 0.0200\n",
      "Epoch [13650/50000], Loss: 0.0200\n",
      "Epoch [13700/50000], Loss: 0.0200\n",
      "Epoch [13750/50000], Loss: 0.0200\n",
      "Epoch [13800/50000], Loss: 0.0200\n",
      "Epoch [13850/50000], Loss: 0.0200\n",
      "Epoch [13900/50000], Loss: 0.0199\n",
      "Epoch [13950/50000], Loss: 0.0199\n",
      "Epoch [14000/50000], Loss: 0.0199\n",
      "Epoch [14050/50000], Loss: 0.0199\n",
      "Epoch [14100/50000], Loss: 0.0199\n",
      "Epoch [14150/50000], Loss: 0.0199\n",
      "Epoch [14200/50000], Loss: 0.0199\n",
      "Epoch [14250/50000], Loss: 0.0198\n",
      "Epoch [14300/50000], Loss: 0.0198\n",
      "Epoch [14350/50000], Loss: 0.0198\n",
      "Epoch [14400/50000], Loss: 0.0198\n",
      "Epoch [14450/50000], Loss: 0.0198\n",
      "Epoch [14500/50000], Loss: 0.0198\n",
      "Epoch [14550/50000], Loss: 0.0198\n",
      "Epoch [14600/50000], Loss: 0.0197\n",
      "Epoch [14650/50000], Loss: 0.0197\n",
      "Epoch [14700/50000], Loss: 0.0197\n",
      "Epoch [14750/50000], Loss: 0.0197\n",
      "Epoch [14800/50000], Loss: 0.0197\n",
      "Epoch [14850/50000], Loss: 0.0197\n",
      "Epoch [14900/50000], Loss: 0.0197\n",
      "Epoch [14950/50000], Loss: 0.0196\n",
      "Epoch [15000/50000], Loss: 0.0196\n",
      "Epoch [15050/50000], Loss: 0.0196\n",
      "Epoch [15100/50000], Loss: 0.0196\n",
      "Epoch [15150/50000], Loss: 0.0196\n",
      "Epoch [15200/50000], Loss: 0.0195\n",
      "Epoch [15250/50000], Loss: 0.0195\n",
      "Epoch [15300/50000], Loss: 0.0195\n",
      "Epoch [15350/50000], Loss: 0.0195\n",
      "Epoch [15400/50000], Loss: 0.0194\n",
      "Epoch [15450/50000], Loss: 0.0194\n",
      "Epoch [15500/50000], Loss: 0.0194\n",
      "Epoch [15550/50000], Loss: 0.0194\n",
      "Epoch [15600/50000], Loss: 0.0193\n",
      "Epoch [15650/50000], Loss: 0.0193\n",
      "Epoch [15700/50000], Loss: 0.0193\n",
      "Epoch [15750/50000], Loss: 0.0192\n",
      "Epoch [15800/50000], Loss: 0.0192\n",
      "Epoch [15850/50000], Loss: 0.0191\n",
      "Epoch [15900/50000], Loss: 0.0191\n",
      "Epoch [15950/50000], Loss: 0.0190\n",
      "Epoch [16000/50000], Loss: 0.0190\n",
      "Epoch [16050/50000], Loss: 0.0189\n",
      "Epoch [16100/50000], Loss: 0.0189\n",
      "Epoch [16150/50000], Loss: 0.0188\n",
      "Epoch [16200/50000], Loss: 0.0188\n",
      "Epoch [16250/50000], Loss: 0.0187\n",
      "Epoch [16300/50000], Loss: 0.0186\n",
      "Epoch [16350/50000], Loss: 0.0186\n",
      "Epoch [16400/50000], Loss: 0.0185\n",
      "Epoch [16450/50000], Loss: 0.0184\n",
      "Epoch [16500/50000], Loss: 0.0184\n",
      "Epoch [16550/50000], Loss: 0.0183\n",
      "Epoch [16600/50000], Loss: 0.0182\n",
      "Epoch [16650/50000], Loss: 0.0182\n",
      "Epoch [16700/50000], Loss: 0.0181\n",
      "Epoch [16750/50000], Loss: 0.0180\n",
      "Epoch [16800/50000], Loss: 0.0180\n",
      "Epoch [16850/50000], Loss: 0.0179\n",
      "Epoch [16900/50000], Loss: 0.0178\n",
      "Epoch [16950/50000], Loss: 0.0177\n",
      "Epoch [17000/50000], Loss: 0.0177\n",
      "Epoch [17050/50000], Loss: 0.0176\n",
      "Epoch [17100/50000], Loss: 0.0176\n",
      "Epoch [17150/50000], Loss: 0.0175\n",
      "Epoch [17200/50000], Loss: 0.0174\n",
      "Epoch [17250/50000], Loss: 0.0174\n",
      "Epoch [17300/50000], Loss: 0.0173\n",
      "Epoch [17350/50000], Loss: 0.0172\n",
      "Epoch [17400/50000], Loss: 0.0172\n",
      "Epoch [17450/50000], Loss: 0.0171\n",
      "Epoch [17500/50000], Loss: 0.0170\n",
      "Epoch [17550/50000], Loss: 0.0170\n",
      "Epoch [17600/50000], Loss: 0.0169\n",
      "Epoch [17650/50000], Loss: 0.0169\n",
      "Epoch [17700/50000], Loss: 0.0168\n",
      "Epoch [17750/50000], Loss: 0.0167\n",
      "Epoch [17800/50000], Loss: 0.0167\n",
      "Epoch [17850/50000], Loss: 0.0166\n",
      "Epoch [17900/50000], Loss: 0.0166\n",
      "Epoch [17950/50000], Loss: 0.0165\n",
      "Epoch [18000/50000], Loss: 0.0164\n",
      "Epoch [18050/50000], Loss: 0.0164\n",
      "Epoch [18100/50000], Loss: 0.0163\n",
      "Epoch [18150/50000], Loss: 0.0162\n",
      "Epoch [18200/50000], Loss: 0.0162\n",
      "Epoch [18250/50000], Loss: 0.0161\n",
      "Epoch [18300/50000], Loss: 0.0160\n",
      "Epoch [18350/50000], Loss: 0.0159\n",
      "Epoch [18400/50000], Loss: 0.0159\n",
      "Epoch [18450/50000], Loss: 0.0158\n",
      "Epoch [18500/50000], Loss: 0.0157\n",
      "Epoch [18550/50000], Loss: 0.0156\n",
      "Epoch [18600/50000], Loss: 0.0155\n",
      "Epoch [18650/50000], Loss: 0.0155\n",
      "Epoch [18700/50000], Loss: 0.0154\n",
      "Epoch [18750/50000], Loss: 0.0153\n",
      "Epoch [18800/50000], Loss: 0.0152\n",
      "Epoch [18850/50000], Loss: 0.0151\n",
      "Epoch [18900/50000], Loss: 0.0150\n",
      "Epoch [18950/50000], Loss: 0.0149\n",
      "Epoch [19000/50000], Loss: 0.0148\n",
      "Epoch [19050/50000], Loss: 0.0147\n",
      "Epoch [19100/50000], Loss: 0.0146\n",
      "Epoch [19150/50000], Loss: 0.0145\n",
      "Epoch [19200/50000], Loss: 0.0144\n",
      "Epoch [19250/50000], Loss: 0.0144\n",
      "Epoch [19300/50000], Loss: 0.0143\n",
      "Epoch [19350/50000], Loss: 0.0142\n",
      "Epoch [19400/50000], Loss: 0.0141\n",
      "Epoch [19450/50000], Loss: 0.0140\n",
      "Epoch [19500/50000], Loss: 0.0139\n",
      "Epoch [19550/50000], Loss: 0.0138\n",
      "Epoch [19600/50000], Loss: 0.0137\n",
      "Epoch [19650/50000], Loss: 0.0136\n",
      "Epoch [19700/50000], Loss: 0.0135\n",
      "Epoch [19750/50000], Loss: 0.0134\n",
      "Epoch [19800/50000], Loss: 0.0133\n",
      "Epoch [19850/50000], Loss: 0.0132\n",
      "Epoch [19900/50000], Loss: 0.0131\n",
      "Epoch [19950/50000], Loss: 0.0130\n",
      "Epoch [20000/50000], Loss: 0.0129\n",
      "Epoch [20050/50000], Loss: 0.0128\n",
      "Epoch [20100/50000], Loss: 0.0127\n",
      "Epoch [20150/50000], Loss: 0.0126\n",
      "Epoch [20200/50000], Loss: 0.0125\n",
      "Epoch [20250/50000], Loss: 0.0124\n",
      "Epoch [20300/50000], Loss: 0.0123\n",
      "Epoch [20350/50000], Loss: 0.0122\n",
      "Epoch [20400/50000], Loss: 0.0121\n",
      "Epoch [20450/50000], Loss: 0.0120\n",
      "Epoch [20500/50000], Loss: 0.0119\n",
      "Epoch [20550/50000], Loss: 0.0118\n",
      "Epoch [20600/50000], Loss: 0.0117\n",
      "Epoch [20650/50000], Loss: 0.0115\n",
      "Epoch [20700/50000], Loss: 0.0114\n",
      "Epoch [20750/50000], Loss: 0.0113\n",
      "Epoch [20800/50000], Loss: 0.0112\n",
      "Epoch [20850/50000], Loss: 0.0111\n",
      "Epoch [20900/50000], Loss: 0.0110\n",
      "Epoch [20950/50000], Loss: 0.0109\n",
      "Epoch [21000/50000], Loss: 0.0108\n",
      "Epoch [21050/50000], Loss: 0.0107\n",
      "Epoch [21100/50000], Loss: 0.0106\n",
      "Epoch [21150/50000], Loss: 0.0105\n",
      "Epoch [21200/50000], Loss: 0.0104\n",
      "Epoch [21250/50000], Loss: 0.0102\n",
      "Epoch [21300/50000], Loss: 0.0101\n",
      "Epoch [21350/50000], Loss: 0.0100\n",
      "Epoch [21400/50000], Loss: 0.0099\n",
      "Epoch [21450/50000], Loss: 0.0098\n",
      "Epoch [21500/50000], Loss: 0.0097\n",
      "Epoch [21550/50000], Loss: 0.0096\n",
      "Epoch [21600/50000], Loss: 0.0095\n",
      "Epoch [21650/50000], Loss: 0.0094\n",
      "Epoch [21700/50000], Loss: 0.0093\n",
      "Epoch [21750/50000], Loss: 0.0092\n",
      "Epoch [21800/50000], Loss: 0.0091\n",
      "Epoch [21850/50000], Loss: 0.0090\n",
      "Epoch [21900/50000], Loss: 0.0089\n",
      "Epoch [21950/50000], Loss: 0.0088\n",
      "Epoch [22000/50000], Loss: 0.0087\n",
      "Epoch [22050/50000], Loss: 0.0086\n",
      "Epoch [22100/50000], Loss: 0.0085\n",
      "Epoch [22150/50000], Loss: 0.0084\n",
      "Epoch [22200/50000], Loss: 0.0083\n",
      "Epoch [22250/50000], Loss: 0.0083\n",
      "Epoch [22300/50000], Loss: 0.0082\n",
      "Epoch [22350/50000], Loss: 0.0081\n",
      "Epoch [22400/50000], Loss: 0.0080\n",
      "Epoch [22450/50000], Loss: 0.0079\n",
      "Epoch [22500/50000], Loss: 0.0078\n",
      "Epoch [22550/50000], Loss: 0.0077\n",
      "Epoch [22600/50000], Loss: 0.0076\n",
      "Epoch [22650/50000], Loss: 0.0075\n",
      "Epoch [22700/50000], Loss: 0.0075\n",
      "Epoch [22750/50000], Loss: 0.0074\n",
      "Epoch [22800/50000], Loss: 0.0073\n",
      "Epoch [22850/50000], Loss: 0.0072\n",
      "Epoch [22900/50000], Loss: 0.0071\n",
      "Epoch [22950/50000], Loss: 0.0070\n",
      "Epoch [23000/50000], Loss: 0.0069\n",
      "Epoch [23050/50000], Loss: 0.0069\n",
      "Epoch [23100/50000], Loss: 0.0068\n",
      "Epoch [23150/50000], Loss: 0.0067\n",
      "Epoch [23200/50000], Loss: 0.0066\n",
      "Epoch [23250/50000], Loss: 0.0065\n",
      "Epoch [23300/50000], Loss: 0.0065\n",
      "Epoch [23350/50000], Loss: 0.0064\n",
      "Epoch [23400/50000], Loss: 0.0063\n",
      "Epoch [23450/50000], Loss: 0.0062\n",
      "Epoch [23500/50000], Loss: 0.0062\n",
      "Epoch [23550/50000], Loss: 0.0061\n",
      "Epoch [23600/50000], Loss: 0.0060\n",
      "Epoch [23650/50000], Loss: 0.0059\n",
      "Epoch [23700/50000], Loss: 0.0059\n",
      "Epoch [23750/50000], Loss: 0.0058\n",
      "Epoch [23800/50000], Loss: 0.0057\n",
      "Epoch [23850/50000], Loss: 0.0056\n",
      "Epoch [23900/50000], Loss: 0.0056\n",
      "Epoch [23950/50000], Loss: 0.0055\n",
      "Epoch [24000/50000], Loss: 0.0054\n",
      "Epoch [24050/50000], Loss: 0.0054\n",
      "Epoch [24100/50000], Loss: 0.0053\n",
      "Epoch [24150/50000], Loss: 0.0052\n",
      "Epoch [24200/50000], Loss: 0.0051\n",
      "Epoch [24250/50000], Loss: 0.0051\n",
      "Epoch [24300/50000], Loss: 0.0050\n",
      "Epoch [24350/50000], Loss: 0.0049\n",
      "Epoch [24400/50000], Loss: 0.0049\n",
      "Epoch [24450/50000], Loss: 0.0048\n",
      "Epoch [24500/50000], Loss: 0.0048\n",
      "Epoch [24550/50000], Loss: 0.0047\n",
      "Epoch [24600/50000], Loss: 0.0046\n",
      "Epoch [24650/50000], Loss: 0.0046\n",
      "Epoch [24700/50000], Loss: 0.0045\n",
      "Epoch [24750/50000], Loss: 0.0044\n",
      "Epoch [24800/50000], Loss: 0.0044\n",
      "Epoch [24850/50000], Loss: 0.0043\n",
      "Epoch [24900/50000], Loss: 0.0043\n",
      "Epoch [24950/50000], Loss: 0.0042\n",
      "Epoch [25000/50000], Loss: 0.0041\n",
      "Epoch [25050/50000], Loss: 0.0041\n",
      "Epoch [25100/50000], Loss: 0.0040\n",
      "Epoch [25150/50000], Loss: 0.0040\n",
      "Epoch [25200/50000], Loss: 0.0039\n",
      "Epoch [25250/50000], Loss: 0.0039\n",
      "Epoch [25300/50000], Loss: 0.0038\n",
      "Epoch [25350/50000], Loss: 0.0038\n",
      "Epoch [25400/50000], Loss: 0.0037\n",
      "Epoch [25450/50000], Loss: 0.0036\n",
      "Epoch [25500/50000], Loss: 0.0036\n",
      "Epoch [25550/50000], Loss: 0.0035\n",
      "Epoch [25600/50000], Loss: 0.0035\n",
      "Epoch [25650/50000], Loss: 0.0034\n",
      "Epoch [25700/50000], Loss: 0.0034\n",
      "Epoch [25750/50000], Loss: 0.0033\n",
      "Epoch [25800/50000], Loss: 0.0033\n",
      "Epoch [25850/50000], Loss: 0.0032\n",
      "Epoch [25900/50000], Loss: 0.0032\n",
      "Epoch [25950/50000], Loss: 0.0032\n",
      "Epoch [26000/50000], Loss: 0.0031\n",
      "Epoch [26050/50000], Loss: 0.0031\n",
      "Epoch [26100/50000], Loss: 0.0030\n",
      "Epoch [26150/50000], Loss: 0.0030\n",
      "Epoch [26200/50000], Loss: 0.0029\n",
      "Epoch [26250/50000], Loss: 0.0029\n",
      "Epoch [26300/50000], Loss: 0.0028\n",
      "Epoch [26350/50000], Loss: 0.0028\n",
      "Epoch [26400/50000], Loss: 0.0028\n",
      "Epoch [26450/50000], Loss: 0.0027\n",
      "Epoch [26500/50000], Loss: 0.0027\n",
      "Epoch [26550/50000], Loss: 0.0026\n",
      "Epoch [26600/50000], Loss: 0.0026\n",
      "Epoch [26650/50000], Loss: 0.0026\n",
      "Epoch [26700/50000], Loss: 0.0025\n",
      "Epoch [26750/50000], Loss: 0.0025\n",
      "Epoch [26800/50000], Loss: 0.0024\n",
      "Epoch [26850/50000], Loss: 0.0024\n",
      "Epoch [26900/50000], Loss: 0.0024\n",
      "Epoch [26950/50000], Loss: 0.0023\n",
      "Epoch [27000/50000], Loss: 0.0023\n",
      "Epoch [27050/50000], Loss: 0.0023\n",
      "Epoch [27100/50000], Loss: 0.0022\n",
      "Epoch [27150/50000], Loss: 0.0022\n",
      "Epoch [27200/50000], Loss: 0.0022\n",
      "Epoch [27250/50000], Loss: 0.0021\n",
      "Epoch [27300/50000], Loss: 0.0021\n",
      "Epoch [27350/50000], Loss: 0.0021\n",
      "Epoch [27400/50000], Loss: 0.0020\n",
      "Epoch [27450/50000], Loss: 0.0020\n",
      "Epoch [27500/50000], Loss: 0.0020\n",
      "Epoch [27550/50000], Loss: 0.0019\n",
      "Epoch [27600/50000], Loss: 0.0019\n",
      "Epoch [27650/50000], Loss: 0.0019\n",
      "Epoch [27700/50000], Loss: 0.0018\n",
      "Epoch [27750/50000], Loss: 0.0018\n",
      "Epoch [27800/50000], Loss: 0.0018\n",
      "Epoch [27850/50000], Loss: 0.0018\n",
      "Epoch [27900/50000], Loss: 0.0017\n",
      "Epoch [27950/50000], Loss: 0.0017\n",
      "Epoch [28000/50000], Loss: 0.0017\n",
      "Epoch [28050/50000], Loss: 0.0016\n",
      "Epoch [28100/50000], Loss: 0.0016\n",
      "Epoch [28150/50000], Loss: 0.0016\n",
      "Epoch [28200/50000], Loss: 0.0016\n",
      "Epoch [28250/50000], Loss: 0.0015\n",
      "Epoch [28300/50000], Loss: 0.0015\n",
      "Epoch [28350/50000], Loss: 0.0015\n",
      "Epoch [28400/50000], Loss: 0.0015\n",
      "Epoch [28450/50000], Loss: 0.0015\n",
      "Epoch [28500/50000], Loss: 0.0014\n",
      "Epoch [28550/50000], Loss: 0.0014\n",
      "Epoch [28600/50000], Loss: 0.0014\n",
      "Epoch [28650/50000], Loss: 0.0014\n",
      "Epoch [28700/50000], Loss: 0.0013\n",
      "Epoch [28750/50000], Loss: 0.0013\n",
      "Epoch [28800/50000], Loss: 0.0013\n",
      "Epoch [28850/50000], Loss: 0.0013\n",
      "Epoch [28900/50000], Loss: 0.0013\n",
      "Epoch [28950/50000], Loss: 0.0012\n",
      "Epoch [29000/50000], Loss: 0.0012\n",
      "Epoch [29050/50000], Loss: 0.0012\n",
      "Epoch [29100/50000], Loss: 0.0012\n",
      "Epoch [29150/50000], Loss: 0.0012\n",
      "Epoch [29200/50000], Loss: 0.0011\n",
      "Epoch [29250/50000], Loss: 0.0011\n",
      "Epoch [29300/50000], Loss: 0.0011\n",
      "Epoch [29350/50000], Loss: 0.0011\n",
      "Epoch [29400/50000], Loss: 0.0011\n",
      "Epoch [29450/50000], Loss: 0.0010\n",
      "Epoch [29500/50000], Loss: 0.0010\n",
      "Epoch [29550/50000], Loss: 0.0010\n",
      "Epoch [29600/50000], Loss: 0.0010\n",
      "Epoch [29650/50000], Loss: 0.0010\n",
      "Epoch [29700/50000], Loss: 0.0010\n",
      "Epoch [29750/50000], Loss: 0.0009\n",
      "Epoch [29800/50000], Loss: 0.0009\n",
      "Epoch [29850/50000], Loss: 0.0009\n",
      "Epoch [29900/50000], Loss: 0.0009\n",
      "Epoch [29950/50000], Loss: 0.0009\n",
      "Epoch [30000/50000], Loss: 0.0009\n",
      "Epoch [30050/50000], Loss: 0.0009\n",
      "Epoch [30100/50000], Loss: 0.0008\n",
      "Epoch [30150/50000], Loss: 0.0008\n",
      "Epoch [30200/50000], Loss: 0.0008\n",
      "Epoch [30250/50000], Loss: 0.0008\n",
      "Epoch [30300/50000], Loss: 0.0008\n",
      "Epoch [30350/50000], Loss: 0.0008\n",
      "Epoch [30400/50000], Loss: 0.0008\n",
      "Epoch [30450/50000], Loss: 0.0008\n",
      "Epoch [30500/50000], Loss: 0.0007\n",
      "Epoch [30550/50000], Loss: 0.0007\n",
      "Epoch [30600/50000], Loss: 0.0007\n",
      "Epoch [30650/50000], Loss: 0.0007\n",
      "Epoch [30700/50000], Loss: 0.0007\n",
      "Epoch [30750/50000], Loss: 0.0007\n",
      "Epoch [30800/50000], Loss: 0.0007\n",
      "Epoch [30850/50000], Loss: 0.0007\n",
      "Epoch [30900/50000], Loss: 0.0006\n",
      "Epoch [30950/50000], Loss: 0.0006\n",
      "Epoch [31000/50000], Loss: 0.0006\n",
      "Epoch [31050/50000], Loss: 0.0006\n",
      "Epoch [31100/50000], Loss: 0.0006\n",
      "Epoch [31150/50000], Loss: 0.0006\n",
      "Epoch [31200/50000], Loss: 0.0006\n",
      "Epoch [31250/50000], Loss: 0.0006\n",
      "Epoch [31300/50000], Loss: 0.0006\n",
      "Epoch [31350/50000], Loss: 0.0006\n",
      "Epoch [31400/50000], Loss: 0.0005\n",
      "Epoch [31450/50000], Loss: 0.0005\n",
      "Epoch [31500/50000], Loss: 0.0005\n",
      "Epoch [31550/50000], Loss: 0.0005\n",
      "Epoch [31600/50000], Loss: 0.0005\n",
      "Epoch [31650/50000], Loss: 0.0005\n",
      "Epoch [31700/50000], Loss: 0.0005\n",
      "Epoch [31750/50000], Loss: 0.0005\n",
      "Epoch [31800/50000], Loss: 0.0005\n",
      "Epoch [31850/50000], Loss: 0.0005\n",
      "Epoch [31900/50000], Loss: 0.0005\n",
      "Epoch [31950/50000], Loss: 0.0005\n",
      "Epoch [32000/50000], Loss: 0.0004\n",
      "Epoch [32050/50000], Loss: 0.0004\n",
      "Epoch [32100/50000], Loss: 0.0004\n",
      "Epoch [32150/50000], Loss: 0.0004\n",
      "Epoch [32200/50000], Loss: 0.0004\n",
      "Epoch [32250/50000], Loss: 0.0004\n",
      "Epoch [32300/50000], Loss: 0.0004\n",
      "Epoch [32350/50000], Loss: 0.0004\n",
      "Epoch [32400/50000], Loss: 0.0004\n",
      "Epoch [32450/50000], Loss: 0.0004\n",
      "Epoch [32500/50000], Loss: 0.0004\n",
      "Epoch [32550/50000], Loss: 0.0004\n",
      "Epoch [32600/50000], Loss: 0.0004\n",
      "Epoch [32650/50000], Loss: 0.0004\n",
      "Epoch [32700/50000], Loss: 0.0004\n",
      "Epoch [32750/50000], Loss: 0.0003\n",
      "Epoch [32800/50000], Loss: 0.0003\n",
      "Epoch [32850/50000], Loss: 0.0003\n",
      "Epoch [32900/50000], Loss: 0.0003\n",
      "Epoch [32950/50000], Loss: 0.0003\n",
      "Epoch [33000/50000], Loss: 0.0003\n",
      "Epoch [33050/50000], Loss: 0.0003\n",
      "Epoch [33100/50000], Loss: 0.0003\n",
      "Epoch [33150/50000], Loss: 0.0003\n",
      "Epoch [33200/50000], Loss: 0.0003\n",
      "Epoch [33250/50000], Loss: 0.0003\n",
      "Epoch [33300/50000], Loss: 0.0003\n",
      "Epoch [33350/50000], Loss: 0.0003\n",
      "Epoch [33400/50000], Loss: 0.0003\n",
      "Epoch [33450/50000], Loss: 0.0003\n",
      "Epoch [33500/50000], Loss: 0.0003\n",
      "Epoch [33550/50000], Loss: 0.0003\n",
      "Epoch [33600/50000], Loss: 0.0003\n",
      "Epoch [33650/50000], Loss: 0.0003\n",
      "Epoch [33700/50000], Loss: 0.0003\n",
      "Epoch [33750/50000], Loss: 0.0002\n",
      "Epoch [33800/50000], Loss: 0.0002\n",
      "Epoch [33850/50000], Loss: 0.0002\n",
      "Epoch [33900/50000], Loss: 0.0002\n",
      "Epoch [33950/50000], Loss: 0.0002\n",
      "Epoch [34000/50000], Loss: 0.0002\n",
      "Epoch [34050/50000], Loss: 0.0002\n",
      "Epoch [34100/50000], Loss: 0.0002\n",
      "Epoch [34150/50000], Loss: 0.0002\n",
      "Epoch [34200/50000], Loss: 0.0002\n",
      "Epoch [34250/50000], Loss: 0.0002\n",
      "Epoch [34300/50000], Loss: 0.0002\n",
      "Epoch [34350/50000], Loss: 0.0002\n",
      "Epoch [34400/50000], Loss: 0.0002\n",
      "Epoch [34450/50000], Loss: 0.0002\n",
      "Epoch [34500/50000], Loss: 0.0002\n",
      "Epoch [34550/50000], Loss: 0.0002\n",
      "Epoch [34600/50000], Loss: 0.0002\n",
      "Epoch [34650/50000], Loss: 0.0002\n",
      "Epoch [34700/50000], Loss: 0.0002\n",
      "Epoch [34750/50000], Loss: 0.0002\n",
      "Epoch [34800/50000], Loss: 0.0002\n",
      "Epoch [34850/50000], Loss: 0.0002\n",
      "Epoch [34900/50000], Loss: 0.0002\n",
      "Epoch [34950/50000], Loss: 0.0002\n",
      "Epoch [35000/50000], Loss: 0.0002\n",
      "Epoch [35050/50000], Loss: 0.0002\n",
      "Epoch [35100/50000], Loss: 0.0002\n",
      "Epoch [35150/50000], Loss: 0.0002\n",
      "Epoch [35200/50000], Loss: 0.0002\n",
      "Epoch [35250/50000], Loss: 0.0001\n",
      "Epoch [35300/50000], Loss: 0.0001\n",
      "Epoch [35350/50000], Loss: 0.0001\n",
      "Epoch [35400/50000], Loss: 0.0001\n",
      "Epoch [35450/50000], Loss: 0.0001\n",
      "Epoch [35500/50000], Loss: 0.0001\n",
      "Epoch [35550/50000], Loss: 0.0001\n",
      "Epoch [35600/50000], Loss: 0.0001\n",
      "Epoch [35650/50000], Loss: 0.0001\n",
      "Epoch [35700/50000], Loss: 0.0001\n",
      "Epoch [35750/50000], Loss: 0.0001\n",
      "Epoch [35800/50000], Loss: 0.0001\n",
      "Epoch [35850/50000], Loss: 0.0001\n",
      "Epoch [35900/50000], Loss: 0.0001\n",
      "Epoch [35950/50000], Loss: 0.0001\n",
      "Epoch [36000/50000], Loss: 0.0001\n",
      "Epoch [36050/50000], Loss: 0.0001\n",
      "Epoch [36100/50000], Loss: 0.0001\n",
      "Epoch [36150/50000], Loss: 0.0001\n",
      "Epoch [36200/50000], Loss: 0.0001\n",
      "Epoch [36250/50000], Loss: 0.0001\n",
      "Epoch [36300/50000], Loss: 0.0001\n",
      "Epoch [36350/50000], Loss: 0.0001\n",
      "Epoch [36400/50000], Loss: 0.0001\n",
      "Epoch [36450/50000], Loss: 0.0001\n",
      "Epoch [36500/50000], Loss: 0.0001\n",
      "Epoch [36550/50000], Loss: 0.0001\n",
      "Epoch [36600/50000], Loss: 0.0001\n",
      "Epoch [36650/50000], Loss: 0.0001\n",
      "Epoch [36700/50000], Loss: 0.0001\n",
      "Epoch [36750/50000], Loss: 0.0001\n",
      "Epoch [36800/50000], Loss: 0.0001\n",
      "Epoch [36850/50000], Loss: 0.0001\n",
      "Epoch [36900/50000], Loss: 0.0001\n",
      "Epoch [36950/50000], Loss: 0.0001\n",
      "Epoch [37000/50000], Loss: 0.0001\n",
      "Epoch [37050/50000], Loss: 0.0001\n",
      "Epoch [37100/50000], Loss: 0.0001\n",
      "Epoch [37150/50000], Loss: 0.0001\n",
      "Epoch [37200/50000], Loss: 0.0001\n",
      "Epoch [37250/50000], Loss: 0.0001\n",
      "Epoch [37300/50000], Loss: 0.0001\n",
      "Epoch [37350/50000], Loss: 0.0001\n",
      "Epoch [37400/50000], Loss: 0.0001\n",
      "Epoch [37450/50000], Loss: 0.0001\n",
      "Epoch [37500/50000], Loss: 0.0001\n",
      "Epoch [37550/50000], Loss: 0.0001\n",
      "Epoch [37600/50000], Loss: 0.0001\n",
      "Epoch [37650/50000], Loss: 0.0001\n",
      "Epoch [37700/50000], Loss: 0.0001\n",
      "Epoch [37750/50000], Loss: 0.0001\n",
      "Epoch [37800/50000], Loss: 0.0001\n",
      "Epoch [37850/50000], Loss: 0.0001\n",
      "Epoch [37900/50000], Loss: 0.0001\n",
      "Epoch [37950/50000], Loss: 0.0001\n",
      "Epoch [38000/50000], Loss: 0.0001\n",
      "Epoch [38050/50000], Loss: 0.0001\n",
      "Epoch [38100/50000], Loss: 0.0001\n",
      "Epoch [38150/50000], Loss: 0.0001\n",
      "Epoch [38200/50000], Loss: 0.0001\n",
      "Epoch [38250/50000], Loss: 0.0001\n",
      "Epoch [38300/50000], Loss: 0.0001\n",
      "Epoch [38350/50000], Loss: 0.0001\n",
      "Epoch [38400/50000], Loss: 0.0001\n",
      "Epoch [38450/50000], Loss: 0.0001\n",
      "Epoch [38500/50000], Loss: 0.0000\n",
      "Epoch [38550/50000], Loss: 0.0000\n",
      "Epoch [38600/50000], Loss: 0.0000\n",
      "Epoch [38650/50000], Loss: 0.0000\n",
      "Epoch [38700/50000], Loss: 0.0000\n",
      "Epoch [38750/50000], Loss: 0.0000\n",
      "Epoch [38800/50000], Loss: 0.0000\n",
      "Epoch [38850/50000], Loss: 0.0000\n",
      "Epoch [38900/50000], Loss: 0.0000\n",
      "Epoch [38950/50000], Loss: 0.0000\n",
      "Epoch [39000/50000], Loss: 0.0000\n",
      "Epoch [39050/50000], Loss: 0.0000\n",
      "Epoch [39100/50000], Loss: 0.0000\n",
      "Epoch [39150/50000], Loss: 0.0000\n",
      "Epoch [39200/50000], Loss: 0.0000\n",
      "Epoch [39250/50000], Loss: 0.0000\n",
      "Epoch [39300/50000], Loss: 0.0000\n",
      "Epoch [39350/50000], Loss: 0.0000\n",
      "Epoch [39400/50000], Loss: 0.0000\n",
      "Epoch [39450/50000], Loss: 0.0000\n",
      "Epoch [39500/50000], Loss: 0.0000\n",
      "Epoch [39550/50000], Loss: 0.0000\n",
      "Epoch [39600/50000], Loss: 0.0000\n",
      "Epoch [39650/50000], Loss: 0.0000\n",
      "Epoch [39700/50000], Loss: 0.0000\n",
      "Epoch [39750/50000], Loss: 0.0000\n",
      "Epoch [39800/50000], Loss: 0.0000\n",
      "Epoch [39850/50000], Loss: 0.0000\n",
      "Epoch [39900/50000], Loss: 0.0000\n",
      "Epoch [39950/50000], Loss: 0.0000\n",
      "Epoch [40000/50000], Loss: 0.0000\n",
      "Epoch [40050/50000], Loss: 0.0000\n",
      "Epoch [40100/50000], Loss: 0.0000\n",
      "Epoch [40150/50000], Loss: 0.0000\n",
      "Epoch [40200/50000], Loss: 0.0000\n",
      "Epoch [40250/50000], Loss: 0.0000\n",
      "Epoch [40300/50000], Loss: 0.0000\n",
      "Epoch [40350/50000], Loss: 0.0000\n",
      "Epoch [40400/50000], Loss: 0.0000\n",
      "Epoch [40450/50000], Loss: 0.0000\n",
      "Epoch [40500/50000], Loss: 0.0000\n",
      "Epoch [40550/50000], Loss: 0.0000\n",
      "Epoch [40600/50000], Loss: 0.0000\n",
      "Epoch [40650/50000], Loss: 0.0000\n",
      "Epoch [40700/50000], Loss: 0.0000\n",
      "Epoch [40750/50000], Loss: 0.0000\n",
      "Epoch [40800/50000], Loss: 0.0000\n",
      "Epoch [40850/50000], Loss: 0.0000\n",
      "Epoch [40900/50000], Loss: 0.0000\n",
      "Epoch [40950/50000], Loss: 0.0000\n",
      "Epoch [41000/50000], Loss: 0.0000\n",
      "Epoch [41050/50000], Loss: 0.0000\n",
      "Epoch [41100/50000], Loss: 0.0000\n",
      "Epoch [41150/50000], Loss: 0.0000\n",
      "Epoch [41200/50000], Loss: 0.0000\n",
      "Epoch [41250/50000], Loss: 0.0000\n",
      "Epoch [41300/50000], Loss: 0.0000\n",
      "Epoch [41350/50000], Loss: 0.0000\n",
      "Epoch [41400/50000], Loss: 0.0000\n",
      "Epoch [41450/50000], Loss: 0.0000\n",
      "Epoch [41500/50000], Loss: 0.0000\n",
      "Epoch [41550/50000], Loss: 0.0000\n",
      "Epoch [41600/50000], Loss: 0.0000\n",
      "Epoch [41650/50000], Loss: 0.0000\n",
      "Epoch [41700/50000], Loss: 0.0000\n",
      "Epoch [41750/50000], Loss: 0.0000\n",
      "Epoch [41800/50000], Loss: 0.0000\n",
      "Epoch [41850/50000], Loss: 0.0000\n",
      "Epoch [41900/50000], Loss: 0.0000\n",
      "Epoch [41950/50000], Loss: 0.0000\n",
      "Epoch [42000/50000], Loss: 0.0000\n",
      "Epoch [42050/50000], Loss: 0.0000\n",
      "Epoch [42100/50000], Loss: 0.0000\n",
      "Epoch [42150/50000], Loss: 0.0000\n",
      "Epoch [42200/50000], Loss: 0.0000\n",
      "Epoch [42250/50000], Loss: 0.0000\n",
      "Epoch [42300/50000], Loss: 0.0000\n",
      "Epoch [42350/50000], Loss: 0.0000\n",
      "Epoch [42400/50000], Loss: 0.0000\n",
      "Epoch [42450/50000], Loss: 0.0000\n",
      "Epoch [42500/50000], Loss: 0.0000\n",
      "Epoch [42550/50000], Loss: 0.0000\n",
      "Epoch [42600/50000], Loss: 0.0000\n",
      "Epoch [42650/50000], Loss: 0.0000\n",
      "Epoch [42700/50000], Loss: 0.0000\n",
      "Epoch [42750/50000], Loss: 0.0000\n",
      "Epoch [42800/50000], Loss: 0.0000\n",
      "Epoch [42850/50000], Loss: 0.0000\n",
      "Epoch [42900/50000], Loss: 0.0000\n",
      "Epoch [42950/50000], Loss: 0.0000\n",
      "Epoch [43000/50000], Loss: 0.0000\n",
      "Epoch [43050/50000], Loss: 0.0000\n",
      "Epoch [43100/50000], Loss: 0.0000\n",
      "Epoch [43150/50000], Loss: 0.0000\n",
      "Epoch [43200/50000], Loss: 0.0000\n",
      "Epoch [43250/50000], Loss: 0.0000\n",
      "Epoch [43300/50000], Loss: 0.0000\n",
      "Epoch [43350/50000], Loss: 0.0000\n",
      "Epoch [43400/50000], Loss: 0.0000\n",
      "Epoch [43450/50000], Loss: 0.0000\n",
      "Epoch [43500/50000], Loss: 0.0000\n",
      "Epoch [43550/50000], Loss: 0.0000\n",
      "Epoch [43600/50000], Loss: 0.0000\n",
      "Epoch [43650/50000], Loss: 0.0000\n",
      "Epoch [43700/50000], Loss: 0.0000\n",
      "Epoch [43750/50000], Loss: 0.0000\n",
      "Epoch [43800/50000], Loss: 0.0000\n",
      "Epoch [43850/50000], Loss: 0.0000\n",
      "Epoch [43900/50000], Loss: 0.0000\n",
      "Epoch [43950/50000], Loss: 0.0000\n",
      "Epoch [44000/50000], Loss: 0.0000\n",
      "Epoch [44050/50000], Loss: 0.0000\n",
      "Epoch [44100/50000], Loss: 0.0000\n",
      "Epoch [44150/50000], Loss: 0.0000\n",
      "Epoch [44200/50000], Loss: 0.0000\n",
      "Epoch [44250/50000], Loss: 0.0000\n",
      "Epoch [44300/50000], Loss: 0.0000\n",
      "Epoch [44350/50000], Loss: 0.0000\n",
      "Epoch [44400/50000], Loss: 0.0000\n",
      "Epoch [44450/50000], Loss: 0.0000\n",
      "Epoch [44500/50000], Loss: 0.0000\n",
      "Epoch [44550/50000], Loss: 0.0000\n",
      "Epoch [44600/50000], Loss: 0.0000\n",
      "Epoch [44650/50000], Loss: 0.0000\n",
      "Epoch [44700/50000], Loss: 0.0000\n",
      "Epoch [44750/50000], Loss: 0.0000\n",
      "Epoch [44800/50000], Loss: 0.0000\n",
      "Epoch [44850/50000], Loss: 0.0000\n",
      "Epoch [44900/50000], Loss: 0.0000\n",
      "Epoch [44950/50000], Loss: 0.0000\n",
      "Epoch [45000/50000], Loss: 0.0000\n",
      "Epoch [45050/50000], Loss: 0.0000\n",
      "Epoch [45100/50000], Loss: 0.0000\n",
      "Epoch [45150/50000], Loss: 0.0000\n",
      "Epoch [45200/50000], Loss: 0.0000\n",
      "Epoch [45250/50000], Loss: 0.0000\n",
      "Epoch [45300/50000], Loss: 0.0000\n",
      "Epoch [45350/50000], Loss: 0.0000\n",
      "Epoch [45400/50000], Loss: 0.0000\n",
      "Epoch [45450/50000], Loss: 0.0000\n",
      "Epoch [45500/50000], Loss: 0.0000\n",
      "Epoch [45550/50000], Loss: 0.0000\n",
      "Epoch [45600/50000], Loss: 0.0000\n",
      "Epoch [45650/50000], Loss: 0.0000\n",
      "Epoch [45700/50000], Loss: 0.0000\n",
      "Epoch [45750/50000], Loss: 0.0000\n",
      "Epoch [45800/50000], Loss: 0.0000\n",
      "Epoch [45850/50000], Loss: 0.0000\n",
      "Epoch [45900/50000], Loss: 0.0000\n",
      "Epoch [45950/50000], Loss: 0.0000\n",
      "Epoch [46000/50000], Loss: 0.0000\n",
      "Epoch [46050/50000], Loss: 0.0000\n",
      "Epoch [46100/50000], Loss: 0.0000\n",
      "Epoch [46150/50000], Loss: 0.0000\n",
      "Epoch [46200/50000], Loss: 0.0000\n",
      "Epoch [46250/50000], Loss: 0.0000\n",
      "Epoch [46300/50000], Loss: 0.0000\n",
      "Epoch [46350/50000], Loss: 0.0000\n",
      "Epoch [46400/50000], Loss: 0.0000\n",
      "Epoch [46450/50000], Loss: 0.0000\n",
      "Epoch [46500/50000], Loss: 0.0000\n",
      "Epoch [46550/50000], Loss: 0.0000\n",
      "Epoch [46600/50000], Loss: 0.0000\n",
      "Epoch [46650/50000], Loss: 0.0000\n",
      "Epoch [46700/50000], Loss: 0.0000\n",
      "Epoch [46750/50000], Loss: 0.0000\n",
      "Epoch [46800/50000], Loss: 0.0000\n",
      "Epoch [46850/50000], Loss: 0.0000\n",
      "Epoch [46900/50000], Loss: 0.0000\n",
      "Epoch [46950/50000], Loss: 0.0000\n",
      "Epoch [47000/50000], Loss: 0.0000\n",
      "Epoch [47050/50000], Loss: 0.0000\n",
      "Epoch [47100/50000], Loss: 0.0000\n",
      "Epoch [47150/50000], Loss: 0.0000\n",
      "Epoch [47200/50000], Loss: 0.0000\n",
      "Epoch [47250/50000], Loss: 0.0000\n",
      "Epoch [47300/50000], Loss: 0.0000\n",
      "Epoch [47350/50000], Loss: 0.0000\n",
      "Epoch [47400/50000], Loss: 0.0000\n",
      "Epoch [47450/50000], Loss: 0.0000\n",
      "Epoch [47500/50000], Loss: 0.0000\n",
      "Epoch [47550/50000], Loss: 0.0000\n",
      "Epoch [47600/50000], Loss: 0.0000\n",
      "Epoch [47650/50000], Loss: 0.0000\n",
      "Epoch [47700/50000], Loss: 0.0000\n",
      "Epoch [47750/50000], Loss: 0.0000\n",
      "Epoch [47800/50000], Loss: 0.0000\n",
      "Epoch [47850/50000], Loss: 0.0000\n",
      "Epoch [47900/50000], Loss: 0.0000\n",
      "Epoch [47950/50000], Loss: 0.0000\n",
      "Epoch [48000/50000], Loss: 0.0000\n",
      "Epoch [48050/50000], Loss: 0.0000\n",
      "Epoch [48100/50000], Loss: 0.0000\n",
      "Epoch [48150/50000], Loss: 0.0000\n",
      "Epoch [48200/50000], Loss: 0.0000\n",
      "Epoch [48250/50000], Loss: 0.0000\n",
      "Epoch [48300/50000], Loss: 0.0000\n",
      "Epoch [48350/50000], Loss: 0.0000\n",
      "Epoch [48400/50000], Loss: 0.0000\n",
      "Epoch [48450/50000], Loss: 0.0000\n",
      "Epoch [48500/50000], Loss: 0.0000\n",
      "Epoch [48550/50000], Loss: 0.0000\n",
      "Epoch [48600/50000], Loss: 0.0000\n",
      "Epoch [48650/50000], Loss: 0.0000\n",
      "Epoch [48700/50000], Loss: 0.0000\n",
      "Epoch [48750/50000], Loss: 0.0000\n",
      "Epoch [48800/50000], Loss: 0.0000\n",
      "Epoch [48850/50000], Loss: 0.0000\n",
      "Epoch [48900/50000], Loss: 0.0000\n",
      "Epoch [48950/50000], Loss: 0.0000\n",
      "Epoch [49000/50000], Loss: 0.0000\n",
      "Epoch [49050/50000], Loss: 0.0000\n",
      "Epoch [49100/50000], Loss: 0.0000\n",
      "Epoch [49150/50000], Loss: 0.0000\n",
      "Epoch [49200/50000], Loss: 0.0000\n",
      "Epoch [49250/50000], Loss: 0.0000\n",
      "Epoch [49300/50000], Loss: 0.0000\n",
      "Epoch [49350/50000], Loss: 0.0000\n",
      "Epoch [49400/50000], Loss: 0.0000\n",
      "Epoch [49450/50000], Loss: 0.0000\n",
      "Epoch [49500/50000], Loss: 0.0000\n",
      "Epoch [49550/50000], Loss: 0.0000\n",
      "Epoch [49600/50000], Loss: 0.0000\n",
      "Epoch [49650/50000], Loss: 0.0000\n",
      "Epoch [49700/50000], Loss: 0.0000\n",
      "Epoch [49750/50000], Loss: 0.0000\n",
      "Epoch [49800/50000], Loss: 0.0000\n",
      "Epoch [49850/50000], Loss: 0.0000\n",
      "Epoch [49900/50000], Loss: 0.0000\n",
      "Epoch [49950/50000], Loss: 0.0000\n",
      "Epoch [50000/50000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# model, loss, and optimizer\n",
    "model = KeyValueMemory(\n",
    "    key_size = key_size,\n",
    "    memory_size = memory_size,\n",
    "    value_size = value_size,\n",
    "    W_keys = init_W_keys,\n",
    "    W_values = init_W_values,\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model()\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (4, 3))\n",
    "plt.plot(losses, label = 'Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize keys and values\n",
    "keys = model.W_keys.detach().numpy()\n",
    "values = model.W_values.detach().numpy()\n",
    "print(keys.shape, values.shape)\n",
    "\n",
    "plt.figure(figsize = (6, 3))\n",
    "\n",
    "# keys\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(keys[:, 0], keys[:, 1])\n",
    "plt.title('Key Representations')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "# plt.legend()\n",
    "\n",
    "# values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(values[:, 0], values[:, 1])\n",
    "plt.title('Value Representations')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Key-value network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, key_size, memory_size):\n",
    "        super(KeyValueNet, self).__init__()\n",
    "        self.fc_key = nn.Linear(input_size, key_size)\n",
    "        self.fc_memory = nn.Linear(memory_size, output_size)\n",
    "        # self.fc_value = nn.Identity()\n",
    "    \n",
    "    def forward(self, key, memory, keys = None, memories = None):\n",
    "        # initialize keys\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "        \n",
    "        # initialize memories\n",
    "        if memories is None:\n",
    "            memories = []\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        self.logits = self.fc_action(x) # record logits for later analyses\n",
    "\n",
    "        # no action masking\n",
    "        if mask is None:\n",
    "            dist = Categorical(logits = self.logits)\n",
    "        \n",
    "        # with action masking\n",
    "        elif mask is not None:\n",
    "            dist = CategoricalMasked(logits = self.logits, mask = mask)\n",
    "        \n",
    "        policy = dist.probs # (batch_size, output_dim)\n",
    "        action = dist.sample() # (batch_size,)\n",
    "        log_prob = dist.log_prob(action) # (batch_size,)\n",
    "        entropy = dist.entropy() # (batch_size,)\n",
    "        \n",
    "        return action, policy, log_prob, entropy\n",
    "\n",
    "    def encode(self, k, v):\n",
    "        key = self.fc_key(k)\n",
    "        value = self.fc_value(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Identity in module torch.nn.modules.linear:\n",
      "\n",
      "class Identity(torch.nn.modules.module.Module)\n",
      " |  Identity(*args: Any, **kwargs: Any) -> None\n",
      " |  \n",
      " |  A placeholder identity operator that is argument-insensitive.\n",
      " |  \n",
      " |  Args:\n",
      " |      args: any argument (unused)\n",
      " |      kwargs: any keyword argument (unused)\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      " |      - Output: :math:`(*)`, same shape as the input.\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n",
      " |      >>> input = torch.randn(128, 20)\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |      torch.Size([128, 20])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Identity\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Define the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): whether to assign items in the state\n",
      " |              dictionary to their corresponding keys in the module instead\n",
      " |              of copying them inplace into the module's current parameters and buffers.\n",
      " |              When ``False``, the properties of the tensors in the current\n",
      " |              module are preserved while when ``True``, the properties of the\n",
      " |              Tensors in the state dict are preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
      " |      \n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Set the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
